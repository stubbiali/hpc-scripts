# HPC2020

This folder contains Bash scripts for the HPC2020 supercomputer @ ECMWF.

The following files are application-agnostic:
* `salloc.sh`, to ask for an interactive allocation on the compute nodes;
* `submit.sh`, to submit a Slurm batch job;
* `clean_home.sh`, to delete all temporary files generated by Slurm in `$HOME`.
* `clean_scratch.sh`, to delete all temporary files generated by Slurm in `$SCRATCH`.

We suggest creating a symlink to the files above in your `$SCRATCH`.

**Remark:** It is intended that *all* files mentioned in this README should be issued
from within `$SCRATCH`. The only exception is `clean_home.sh`, which should be executed
inside `$HOME`.

The following applications are fully supported:
* FVM
* CLOUDSC

Below you can find detailed instructions to get started with each supported application.


## FVM

It is assumed that you have cloned the branch `$BRANCH` of the FVM repository under
`$SCRATH/fvm-gt4py/$BRANCH`. The following files are available:

* `install_fvm.sh`, to install FVM and all its dependencies;
* `prepare_fvm.sh`, to load all the modules required to run the model and set the path to the
source code;
* `run_fvm.sh`, to run the model;
* `submit_fvm.sh`, to run the model from within Slurm jobs.

You can set the programming environment via the variable `ENV` (options are `amd`, `gnu` and `intel`)
and the MPI library via `MPI` (options are `hpcx`, `intel`, and `openmpi`). It then follows that:

* The virtual environment is placed under `$SCRATH/fvm-gt4py/$BRANCH/venv/$ENV-$MPI`;
* The GT4Py cache directory is `$SCRATH/fvm-gt4py/$BRANCH/gt_cache/$ENV`;
* The build directory for GHEX is `$SCRATH/fvm-gt4py/$BRANCH/_externals/ghex/build/$MPI/cpu`.


## CLOUDSC

[TODO]
